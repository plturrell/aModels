#!/bin/bash
set -e

# Deployment script for wiring models into aModels system

echo "ğŸš€ Deploying Models into aModels System"
echo ""

# Check LocalAI is running
echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"
echo "Step 1: Verify LocalAI Service"
echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"

if curl -s http://localhost:8080/v1/models > /dev/null 2>&1; then
    echo "âœ… LocalAI is running"
    curl -s http://localhost:8080/v1/models | head -c 200
    echo ""
else
    echo "âŒ LocalAI is not running"
    echo "   Starting LocalAI..."
    cd /home/aModels/services/localai
    ./start-production.sh
    sleep 5
fi

# Check Gateway integration
echo ""
echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"
echo "Step 2: Verify Gateway Integration"
echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"

if curl -s http://localhost:8000/healthz > /dev/null 2>&1; then
    echo "âœ… Gateway is running"
    LOCALAI_STATUS=$(curl -s http://localhost:8000/healthz | grep -o '"localai":"[^"]*"' | cut -d'"' -f4)
    echo "   LocalAI status: $LOCALAI_STATUS"
else
    echo "âš ï¸  Gateway is not running"
    echo "   Start with: cd services/gateway && ./start.sh"
fi

# Test model inference
echo ""
echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"
echo "Step 3: Test Model Inference"
echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"

echo "Testing VaultGemma model..."
RESPONSE=$(curl -s http://localhost:8080/v1/chat/completions \
    -H 'Content-Type: application/json' \
    -d '{"model":"vaultgemma","messages":[{"role":"user","content":"Hello"}],"max_tokens":32}')

if echo "$RESPONSE" | grep -q "choices\|content"; then
    echo "âœ… Model inference working"
    echo "$RESPONSE" | head -c 200
    echo ""
else
    echo "âš ï¸  Model inference may be in stub mode"
    echo "   Response: $RESPONSE"
fi

# Check environment variables
echo ""
echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"
echo "Step 4: Environment Configuration"
echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"

echo "LOCALAI_URL: ${LOCALAI_URL:-http://localhost:8080}"
echo "GATEWAY_PORT: ${GATEWAY_PORT:-8000}"

# Summary
echo ""
echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"
echo "âœ… Deployment Check Complete"
echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"
echo ""
echo "Next steps:"
echo "  1. Verify all services can reach LocalAI"
echo "  2. Update service configs with LOCALAI_URL"
echo "  3. Test end-to-end workflows"
echo "  4. Monitor model performance"


