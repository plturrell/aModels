//go:build hana

package storage

import (
	"bytes"
	"context"
	"crypto/sha256"
	"database/sql"
	"encoding/hex"
	"encoding/json"
	"fmt"
	"log"
	"strings"
	"time"

	hdbdriver "github.com/SAP/go-hdb/driver"

	"github.com/plturrell/agenticAiETH/agenticAiETH_layer4_LocalAI/pkg/hanapool"
)

// SemanticCacheEntry represents a semantically cached inference response
type SemanticCacheEntry struct {
	ID              int64             `json:"id"`
	CacheKey        string            `json:"cache_key"`
	PromptHash      string            `json:"prompt_hash"`
	SemanticHash    string            `json:"semantic_hash"`
	Model           string            `json:"model"`
	Domain          string            `json:"domain"`
	Prompt          string            `json:"prompt"`
	Response        string            `json:"response"`
	TokensUsed      int               `json:"tokens_used"`
	Temperature     float64           `json:"temperature"`
	MaxTokens       int               `json:"max_tokens"`
	SimilarityScore float64           `json:"similarity_score"`
	CreatedAt       time.Time         `json:"created_at"`
	ExpiresAt       time.Time         `json:"expires_at"`
	AccessCount     int               `json:"access_count"`
	LastAccessed    time.Time         `json:"last_accessed"`
	Metadata        map[string]string `json:"metadata"`
	Tags            []string          `json:"tags"`
}

// SemanticCacheConfig holds configuration for semantic caching
type SemanticCacheConfig struct {
	DefaultTTL          time.Duration `json:"default_ttl"`
	SimilarityThreshold float64       `json:"similarity_threshold"`
	MaxEntries          int           `json:"max_entries"`
	CleanupInterval     time.Duration `json:"cleanup_interval"`
	EnableVectorSearch  bool          `json:"enable_vector_search"`
	EnableFuzzyMatching bool          `json:"enable_fuzzy_matching"`
}

// SemanticCacheStats tracks semantic cache performance
type SemanticCacheStats struct {
	TotalEntries       int64            `json:"total_entries"`
	HitCount           int64            `json:"hit_count"`
	MissCount          int64            `json:"miss_count"`
	SemanticHitCount   int64            `json:"semantic_hit_count"`
	HitRate            float64          `json:"hit_rate"`
	SemanticHitRate    float64          `json:"semantic_hit_rate"`
	AvgSimilarityScore float64          `json:"avg_similarity_score"`
	ExpiredEntries     int64            `json:"expired_entries"`
	ByModel            map[string]int64 `json:"by_model"`
	ByDomain           map[string]int64 `json:"by_domain"`
	TopTags            map[string]int64 `json:"top_tags"`
}

// SemanticCache provides advanced semantic caching for LocalAI inference
type SemanticCache struct {
	pool   *hanapool.Pool
	config *SemanticCacheConfig
}

// NewSemanticCache creates a new semantic cache
func NewSemanticCache(pool *hanapool.Pool, config *SemanticCacheConfig) *SemanticCache {
	if config == nil {
		config = &SemanticCacheConfig{
			DefaultTTL:          24 * time.Hour,
			SimilarityThreshold: 0.8,
			MaxEntries:          10000,
			CleanupInterval:     1 * time.Hour,
			EnableVectorSearch:  false,
			EnableFuzzyMatching: true,
		}
	}

	return &SemanticCache{
		pool:   pool,
		config: config,
	}
}

func (sc *SemanticCache) poolAvailable() bool {
	if sc == nil || sc.pool == nil {
		return false
	}
	return sc.pool.GetDB() != nil
}

// CreateTables creates the semantic cache tables
func (sc *SemanticCache) CreateTables(ctx context.Context) error {
	if !sc.poolAvailable() {
		return fmt.Errorf("hana pool not configured")
	}

	createEntries := `
CREATE COLUMN TABLE semantic_cache_entries (
	id BIGINT GENERATED BY DEFAULT AS IDENTITY PRIMARY KEY,
	cache_key NVARCHAR(255) NOT NULL UNIQUE,
	prompt_hash NVARCHAR(64) NOT NULL,
	semantic_hash NVARCHAR(64) NOT NULL,
	model NVARCHAR(100) NOT NULL,
	domain NVARCHAR(100),
	prompt NCLOB,
	response NCLOB NOT NULL,
	tokens_used INTEGER,
	temperature DECIMAL(5,2),
	max_tokens INTEGER,
	similarity_score DECIMAL(3,2) DEFAULT 1.0,
	created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
	expires_at TIMESTAMP,
	access_count INTEGER DEFAULT 0,
	last_accessed TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
	metadata NCLOB,
	tags NCLOB
)`
	if _, err := sc.pool.Execute(ctx, createEntries); err != nil && !isAlreadyExistsError(err) {
		return fmt.Errorf("failed to create semantic_cache_entries table: %w", err)
	}

	createStats := `
CREATE COLUMN TABLE semantic_cache_stats (
	id BIGINT GENERATED BY DEFAULT AS IDENTITY PRIMARY KEY,
	stat_name NVARCHAR(100) NOT NULL,
	stat_value BIGINT NOT NULL,
	stat_type NVARCHAR(50) NOT NULL,
	recorded_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
)`
	if _, err := sc.pool.Execute(ctx, createStats); err != nil && !isAlreadyExistsError(err) {
		return fmt.Errorf("failed to create semantic_cache_stats table: %w", err)
	}

	indexes := []string{
		"CREATE INDEX idx_semantic_cache_key ON semantic_cache_entries (cache_key)",
		"CREATE INDEX idx_semantic_prompt_hash ON semantic_cache_entries (prompt_hash)",
		"CREATE INDEX idx_semantic_hash ON semantic_cache_entries (semantic_hash)",
		"CREATE INDEX idx_semantic_model ON semantic_cache_entries (model)",
		"CREATE INDEX idx_semantic_domain ON semantic_cache_entries (domain)",
		"CREATE INDEX idx_semantic_expires ON semantic_cache_entries (expires_at)",
		"CREATE INDEX idx_semantic_similarity ON semantic_cache_entries (similarity_score)",
		"CREATE INDEX idx_semantic_access ON semantic_cache_entries (access_count)",
		"CREATE INDEX idx_semantic_last_accessed ON semantic_cache_entries (last_accessed)",
		"CREATE INDEX idx_semantic_stat_name ON semantic_cache_stats (stat_name)",
		"CREATE INDEX idx_semantic_stat_type ON semantic_cache_stats (stat_type)",
		"CREATE INDEX idx_semantic_recorded_at ON semantic_cache_stats (recorded_at)",
	}

	for _, stmt := range indexes {
		if _, err := sc.pool.Execute(ctx, stmt); err != nil && !isAlreadyExistsError(err) {
			fmt.Printf("âš ï¸  Semantic cache index creation failed: %v\n", err)
		}
	}

	log.Println("âœ… Semantic cache tables verified successfully")
	return nil
}

// GenerateSemanticHash creates a semantic hash for similarity matching
func (sc *SemanticCache) GenerateSemanticHash(prompt string) string {
	// Normalize prompt for semantic matching
	normalized := strings.ToLower(strings.TrimSpace(prompt))

	// Remove common stop words and normalize
	words := strings.Fields(normalized)
	var filteredWords []string

	stopWords := map[string]bool{
		"the": true, "a": true, "an": true, "and": true, "or": true, "but": true,
		"in": true, "on": true, "at": true, "to": true, "for": true, "of": true,
		"with": true, "by": true, "is": true, "are": true, "was": true, "were": true,
		"be": true, "been": true, "have": true, "has": true, "had": true, "do": true,
		"does": true, "did": true, "will": true, "would": true, "could": true, "should": true,
	}

	for _, word := range words {
		if !stopWords[word] && len(word) > 2 {
			filteredWords = append(filteredWords, word)
		}
	}

	// Create semantic hash from filtered words
	semanticText := strings.Join(filteredWords, " ")
	hash := sha256.Sum256([]byte(semanticText))
	return hex.EncodeToString(hash[:])
}

// GenerateCacheKey creates a cache key from prompt and parameters
func (sc *SemanticCache) GenerateCacheKey(prompt, model, domain string, temperature float64, maxTokens int, topP float64, topK int) string {
	// Create hash of prompt and parameters
	keyData := fmt.Sprintf("%s:%s:%s:%.2f:%d:%.3f:%d", prompt, model, domain, temperature, maxTokens, topP, topK)
	keyHash := sha256.Sum256([]byte(keyData))
	return hex.EncodeToString(keyHash[:])
}

// Get retrieves a cached response
func (sc *SemanticCache) Get(ctx context.Context, cacheKey string) (*SemanticCacheEntry, error) {
	if !sc.poolAvailable() {
		return nil, fmt.Errorf("hana pool not configured")
	}

	query := `
		SELECT id, cache_key, prompt_hash, semantic_hash, model, domain, prompt,
		       response, tokens_used, temperature, max_tokens, similarity_score,
		       created_at, expires_at, access_count, last_accessed, metadata, tags
		FROM semantic_cache_entries 
		WHERE cache_key = ? AND (expires_at IS NULL OR expires_at > CURRENT_TIMESTAMP)
	`

	row := sc.pool.QueryRow(ctx, query, cacheKey)

	entry, err := scanSemanticCacheEntry(row)
	if err != nil {
		if err == sql.ErrNoRows {
			return nil, nil
		}
		return nil, fmt.Errorf("failed to get cache entry: %w", err)
	}

	// Update access statistics
	go sc.updateAccessStats(context.Background(), entry.ID)

	return entry, nil
}

// Set stores a response in cache
func (sc *SemanticCache) Set(ctx context.Context, entry *SemanticCacheEntry) error {
	if !sc.poolAvailable() {
		return fmt.Errorf("hana pool not configured")
	}

	if entry.CreatedAt.IsZero() {
		entry.CreatedAt = time.Now()
	}
	if entry.ExpiresAt.IsZero() {
		entry.ExpiresAt = time.Now().Add(sc.config.DefaultTTL)
	}
	if entry.LastAccessed.IsZero() {
		entry.LastAccessed = entry.CreatedAt
	}
	if entry.SimilarityScore == 0 {
		entry.SimilarityScore = 1.0
	}
	if entry.SemanticHash == "" {
		entry.SemanticHash = sc.GenerateSemanticHash(entry.Prompt)
	}

	metadataJSON, _ := json.Marshal(entry.Metadata)
	tagsJSON, _ := json.Marshal(entry.Tags)

	query := `
UPSERT semantic_cache_entries (
	cache_key, prompt_hash, semantic_hash, model, domain, prompt,
	response, tokens_used, temperature, max_tokens, similarity_score,
	created_at, expires_at, access_count, last_accessed, metadata, tags
) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
WITH PRIMARY KEY
`

	_, err := sc.pool.Execute(ctx, query,
		entry.CacheKey,
		entry.PromptHash,
		entry.SemanticHash,
		entry.Model,
		entry.Domain,
		entry.Prompt,
		entry.Response,
		entry.TokensUsed,
		entry.Temperature,
		entry.MaxTokens,
		entry.SimilarityScore,
		entry.CreatedAt,
		entry.ExpiresAt,
		entry.AccessCount,
		entry.LastAccessed,
		string(metadataJSON),
		string(tagsJSON),
	)

	if err != nil {
		return fmt.Errorf("failed to set cache entry: %w", err)
	}

	return nil
}

// FindSemanticSimilar finds semantically similar cached responses
func (sc *SemanticCache) FindSemanticSimilar(ctx context.Context, prompt, model, domain string, threshold float64, limit int) ([]*SemanticCacheEntry, error) {
	if !sc.poolAvailable() {
		return nil, fmt.Errorf("hana pool not configured")
	}

	// Generate semantic hash for the prompt
	semanticHash := sc.GenerateSemanticHash(prompt)

	// Find entries with similar semantic hash
	query := `
		SELECT id, cache_key, prompt_hash, semantic_hash, model, domain, prompt,
		       response, tokens_used, temperature, max_tokens, similarity_score,
		       created_at, expires_at, access_count, last_accessed, metadata, tags
		FROM semantic_cache_entries 
		WHERE model = ? AND domain = ? 
		  AND (expires_at IS NULL OR expires_at > CURRENT_TIMESTAMP)
		  AND (
			semantic_hash = ? OR
			LOWER(prompt) LIKE ? OR
			LOWER(response) LIKE ?
		  )
		ORDER BY similarity_score DESC, access_count DESC, created_at DESC
		LIMIT ?
	`

	// Create search patterns for fuzzy matching
	words := strings.Fields(strings.ToLower(strings.TrimSpace(prompt)))
	pattern1 := "%" + strings.Join(words[:minInt(3, len(words))], "%") + "%"
	pattern2 := "%" + strings.Join(words[:minInt(5, len(words))], "%") + "%"

	rows, err := sc.pool.Query(ctx, query, model, domain, semanticHash, pattern1, pattern2, limit)
	if err != nil {
		return nil, fmt.Errorf("failed to find similar cache entries: %w", err)
	}
	defer rows.Close()

	var entries []*SemanticCacheEntry
	for rows.Next() {
		entry := &SemanticCacheEntry{}
		var metadataJSON, tagsJSON string

		err := rows.Scan(
			&entry.ID,
			&entry.CacheKey,
			&entry.PromptHash,
			&entry.SemanticHash,
			&entry.Model,
			&entry.Domain,
			&entry.Prompt,
			&entry.Response,
			&entry.TokensUsed,
			&entry.Temperature,
			&entry.MaxTokens,
			&entry.SimilarityScore,
			&entry.CreatedAt,
			&entry.ExpiresAt,
			&entry.AccessCount,
			&entry.LastAccessed,
			&metadataJSON,
			&tagsJSON,
		)
		if err != nil {
			return nil, fmt.Errorf("failed to scan cache entry: %w", err)
		}

		// Parse JSON fields
		if metadataJSON != "" {
			json.Unmarshal([]byte(metadataJSON), &entry.Metadata)
		}
		if tagsJSON != "" {
			json.Unmarshal([]byte(tagsJSON), &entry.Tags)
		}

		// Calculate similarity score if not set
		if entry.SimilarityScore == 0 {
			entry.SimilarityScore = sc.calculateSimilarity(prompt, entry.Prompt)
		}

		// Filter by threshold
		if entry.SimilarityScore >= threshold {
			entries = append(entries, entry)
		}
	}

	return entries, nil
}

// calculateSimilarity calculates text similarity using Jaccard similarity
func (sc *SemanticCache) calculateSimilarity(text1, text2 string) float64 {
	// Normalize texts
	words1 := strings.Fields(strings.ToLower(strings.TrimSpace(text1)))
	words2 := strings.Fields(strings.ToLower(strings.TrimSpace(text2)))

	// Create word sets
	set1 := make(map[string]bool)
	set2 := make(map[string]bool)

	for _, word := range words1 {
		if len(word) > 2 { // Filter short words
			set1[word] = true
		}
	}

	for _, word := range words2 {
		if len(word) > 2 { // Filter short words
			set2[word] = true
		}
	}

	// Calculate intersection and union
	intersection := 0
	for word := range set1 {
		if set2[word] {
			intersection++
		}
	}

	union := len(set1) + len(set2) - intersection

	if union == 0 {
		return 0.0
	}

	return float64(intersection) / float64(union)
}

// GetStats retrieves semantic cache performance statistics
func (sc *SemanticCache) GetStats(ctx context.Context) (*SemanticCacheStats, error) {
	if !sc.poolAvailable() {
		return nil, fmt.Errorf("hana pool not configured")
	}

	query := `
		SELECT 
			COUNT(*) as total_entries,
			SUM(access_count) as total_accesses,
			AVG(access_count) as avg_access_count,
			AVG(similarity_score) as avg_similarity_score,
			COUNT(CASE WHEN expires_at < CURRENT_TIMESTAMP THEN 1 END) as expired_entries
		FROM semantic_cache_entries
	`

	row := sc.pool.QueryRow(ctx, query)

	stats := &SemanticCacheStats{
		ByModel:  make(map[string]int64),
		ByDomain: make(map[string]int64),
		TopTags:  make(map[string]int64),
	}

	var totalAccesses, avgAccessCount, avgSimilarityScore, expiredEntries float64

	err := row.Scan(
		&stats.TotalEntries,
		&totalAccesses,
		&avgAccessCount,
		&avgSimilarityScore,
		&expiredEntries,
	)

	if err != nil {
		return nil, fmt.Errorf("failed to get cache stats: %w", err)
	}

	stats.HitCount = int64(totalAccesses)
	stats.AvgSimilarityScore = avgSimilarityScore

	// Calculate hit rate (simplified)
	if stats.TotalEntries > 0 {
		stats.HitRate = float64(stats.HitCount) / float64(stats.TotalEntries)
	}

	// Get stats by model
	modelQuery := `
		SELECT model, COUNT(*) as count
		FROM semantic_cache_entries
		WHERE (expires_at IS NULL OR expires_at > CURRENT_TIMESTAMP)
		GROUP BY model
	`

	rows, err := sc.pool.Query(ctx, modelQuery)
	if err == nil {
		defer rows.Close()
		for rows.Next() {
			var model string
			var count int64
			if err := rows.Scan(&model, &count); err == nil {
				stats.ByModel[model] = count
			}
		}
	}

	// Get stats by domain
	domainQuery := `
		SELECT domain, COUNT(*) as count
		FROM semantic_cache_entries
		WHERE (expires_at IS NULL OR expires_at > CURRENT_TIMESTAMP)
		GROUP BY domain
	`

	rows, err = sc.pool.Query(ctx, domainQuery)
	if err == nil {
		defer rows.Close()
		for rows.Next() {
			var domain string
			var count int64
			if err := rows.Scan(&domain, &count); err == nil {
				stats.ByDomain[domain] = count
			}
		}
	}

	return stats, nil
}

// CleanupExpired removes expired cache entries
func (sc *SemanticCache) CleanupExpired(ctx context.Context) error {
	if !sc.poolAvailable() {
		return fmt.Errorf("hana pool not configured")
	}

	query := `
		DELETE FROM semantic_cache_entries 
		WHERE expires_at < CURRENT_TIMESTAMP
	`

	result, err := sc.pool.Execute(ctx, query)
	if err != nil {
		return fmt.Errorf("failed to cleanup expired entries: %w", err)
	}

	rowsAffected, _ := result.RowsAffected()
	log.Printf("ðŸ§¹ Cleaned up %d expired semantic cache entries", rowsAffected)

	return nil
}

// CleanupOldEntries removes old cache entries based on access count
func (sc *SemanticCache) CleanupOldEntries(ctx context.Context, olderThanDays int, minAccessCount int) error {
	if !sc.poolAvailable() {
		return fmt.Errorf("hana pool not configured")
	}

	query := `
		DELETE FROM semantic_cache_entries 
		WHERE created_at < DATEADD(day, -?, CURRENT_TIMESTAMP)
		  AND access_count < ?
	`

	result, err := sc.pool.Execute(ctx, query, olderThanDays, minAccessCount)
	if err != nil {
		return fmt.Errorf("failed to cleanup old entries: %w", err)
	}

	rowsAffected, _ := result.RowsAffected()
	log.Printf("ðŸ§¹ Cleaned up %d old semantic cache entries", rowsAffected)

	return nil
}

// GetTopEntries retrieves the most accessed cache entries
func (sc *SemanticCache) GetTopEntries(ctx context.Context, limit int) ([]*SemanticCacheEntry, error) {
	if !sc.poolAvailable() {
		return nil, fmt.Errorf("hana pool not configured")
	}

	query := `
		SELECT id, cache_key, prompt_hash, semantic_hash, model, domain, prompt,
		       response, tokens_used, temperature, max_tokens, similarity_score,
		       created_at, expires_at, access_count, last_accessed, metadata, tags
		FROM semantic_cache_entries 
		WHERE (expires_at IS NULL OR expires_at > CURRENT_TIMESTAMP)
		ORDER BY access_count DESC, similarity_score DESC, last_accessed DESC
		LIMIT ?
	`

	rows, err := sc.pool.Query(ctx, query, limit)
	if err != nil {
		return nil, fmt.Errorf("failed to get top entries: %w", err)
	}
	defer rows.Close()

	var entries []*SemanticCacheEntry
	for rows.Next() {
		entry := &SemanticCacheEntry{}
		var metadataJSON, tagsJSON string

		err := rows.Scan(
			&entry.ID,
			&entry.CacheKey,
			&entry.PromptHash,
			&entry.SemanticHash,
			&entry.Model,
			&entry.Domain,
			&entry.Prompt,
			&entry.Response,
			&entry.TokensUsed,
			&entry.Temperature,
			&entry.MaxTokens,
			&entry.SimilarityScore,
			&entry.CreatedAt,
			&entry.ExpiresAt,
			&entry.AccessCount,
			&entry.LastAccessed,
			&metadataJSON,
			&tagsJSON,
		)
		if err != nil {
			return nil, fmt.Errorf("failed to scan cache entry: %w", err)
		}

		// Parse JSON fields
		if metadataJSON != "" {
			json.Unmarshal([]byte(metadataJSON), &entry.Metadata)
		}
		if tagsJSON != "" {
			json.Unmarshal([]byte(tagsJSON), &entry.Tags)
		}

		entries = append(entries, entry)
	}

	return entries, nil
}

// updateAccessStats updates access statistics for a cache entry
func (sc *SemanticCache) updateAccessStats(ctx context.Context, entryID int64) {
	if !sc.poolAvailable() {
		return
	}

	query := `
		UPDATE semantic_cache_entries 
		SET access_count = access_count + 1,
		    last_accessed = CURRENT_TIMESTAMP
		WHERE id = ?
	`

	_, err := sc.pool.Execute(ctx, query, entryID)
	if err != nil {
		log.Printf("Failed to update access stats: %v", err)
	}
}

// GetByTags retrieves cache entries by tags
func (sc *SemanticCache) GetByTags(ctx context.Context, tags []string, limit int) ([]*SemanticCacheEntry, error) {
	if !sc.poolAvailable() {
		return nil, fmt.Errorf("hana pool not configured")
	}

	if len(tags) == 0 {
		return nil, fmt.Errorf("no tags provided")
	}

	// Build tag filter
	tagFilters := make([]string, len(tags))
	for i := range tags {
		tagFilters[i] = "JSON_CONTAINS(tags, ?)"
	}

	query := fmt.Sprintf(`
		SELECT id, cache_key, prompt_hash, semantic_hash, model, domain, prompt,
		       response, tokens_used, temperature, max_tokens, similarity_score,
		       created_at, expires_at, access_count, last_accessed, metadata, tags
		FROM semantic_cache_entries
		WHERE (%s) AND (expires_at IS NULL OR expires_at > CURRENT_TIMESTAMP)
		ORDER BY access_count DESC, similarity_score DESC
		LIMIT ?
	`, strings.Join(tagFilters, " OR "))

	args := make([]interface{}, len(tags)+1)
	for i, tag := range tags {
		args[i] = fmt.Sprintf(`"%s"`, tag)
	}
	args[len(tags)] = limit

	rows, err := sc.pool.Query(ctx, query, args...)
	if err != nil {
		return nil, fmt.Errorf("failed to get entries by tags: %w", err)
	}
	defer rows.Close()

	var entries []*SemanticCacheEntry
	for rows.Next() {
		entry := &SemanticCacheEntry{}
		var metadataJSON, tagsJSON string

		err := rows.Scan(
			&entry.ID,
			&entry.CacheKey,
			&entry.PromptHash,
			&entry.SemanticHash,
			&entry.Model,
			&entry.Domain,
			&entry.Prompt,
			&entry.Response,
			&entry.TokensUsed,
			&entry.Temperature,
			&entry.MaxTokens,
			&entry.SimilarityScore,
			&entry.CreatedAt,
			&entry.ExpiresAt,
			&entry.AccessCount,
			&entry.LastAccessed,
			&metadataJSON,
			&tagsJSON,
		)
		if err != nil {
			return nil, fmt.Errorf("failed to scan cache entry: %w", err)
		}

		// Parse JSON fields
		if metadataJSON != "" {
			json.Unmarshal([]byte(metadataJSON), &entry.Metadata)
		}
		if tagsJSON != "" {
			json.Unmarshal([]byte(tagsJSON), &entry.Tags)
		}

		entries = append(entries, entry)
	}

	return entries, nil
}

// Helper function
type rowScanner interface {
	Scan(dest ...any) error
}

func scanSemanticCacheEntry(scanner rowScanner) (*SemanticCacheEntry, error) {
	entry := &SemanticCacheEntry{}
	var metadataJSON, tagsJSON string
	var promptBuf, responseBuf bytes.Buffer
	promptLob := hdbdriver.NewLob(nil, &promptBuf)
	responseLob := hdbdriver.NewLob(nil, &responseBuf)

	if err := scanner.Scan(
		&entry.ID,
		&entry.CacheKey,
		&entry.PromptHash,
		&entry.SemanticHash,
		&entry.Model,
		&entry.Domain,
		promptLob,
		responseLob,
		&entry.TokensUsed,
		&entry.Temperature,
		&entry.MaxTokens,
		&entry.SimilarityScore,
		&entry.CreatedAt,
		&entry.ExpiresAt,
		&entry.AccessCount,
		&entry.LastAccessed,
		&metadataJSON,
		&tagsJSON,
	); err != nil {
		return nil, err
	}

	entry.Prompt = promptBuf.String()
	entry.Response = responseBuf.String()

	if metadataJSON != "" {
		_ = json.Unmarshal([]byte(metadataJSON), &entry.Metadata)
	}
	if tagsJSON != "" {
		_ = json.Unmarshal([]byte(tagsJSON), &entry.Tags)
	}

	return entry, nil
}

func minInt(a, b int) int {
	if a < b {
		return a
	}
	return b
}
