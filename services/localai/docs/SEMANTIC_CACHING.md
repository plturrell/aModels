# Semantic Caching for LocalAI

## Overview

Semantic caching is an advanced caching mechanism that goes beyond exact key matching to find semantically similar cached responses. This dramatically improves cache hit rates by recognizing when different prompts are asking for similar information, even if they use different wording.

## Key Features

### ðŸ§  **Intelligent Similarity Matching**
- **Semantic Hash Generation**: Creates semantic hashes that group similar prompts together
- **Fuzzy Matching**: Finds responses to similar questions even with different wording
- **Similarity Scoring**: Ranks cached responses by semantic similarity
- **Threshold-based Filtering**: Only returns responses above a configurable similarity threshold

### ðŸš€ **Performance Benefits**
- **Higher Cache Hit Rates**: Dramatically increases cache effectiveness
- **Reduced Inference Costs**: Avoids expensive model inference for similar queries
- **Faster Response Times**: Returns cached responses in milliseconds
- **Bandwidth Savings**: Reduces network traffic and computational load

### ðŸ“Š **Advanced Analytics**
- **Similarity Metrics**: Tracks similarity scores and hit rates
- **Usage Patterns**: Analyzes which types of queries benefit most from semantic caching
- **Performance Monitoring**: Comprehensive statistics and monitoring
- **Tag-based Organization**: Categorizes cached responses for better organization

## Architecture

### Core Components

#### 1. SemanticCacheEntry
```go
type SemanticCacheEntry struct {
    ID              int64             `json:"id"`
    CacheKey        string            `json:"cache_key"`
    PromptHash      string            `json:"prompt_hash"`
    SemanticHash    string            `json:"semantic_hash"`
    Model           string            `json:"model"`
    Domain          string            `json:"domain"`
    Prompt          string            `json:"prompt"`
    Response        string            `json:"response"`
    TokensUsed      int               `json:"tokens_used"`
    Temperature     float64           `json:"temperature"`
    MaxTokens       int               `json:"max_tokens"`
    SimilarityScore float64           `json:"similarity_score"`
    CreatedAt       time.Time         `json:"created_at"`
    ExpiresAt       time.Time         `json:"expires_at"`
    AccessCount     int               `json:"access_count"`
    LastAccessed    time.Time         `json:"last_accessed"`
    Metadata        map[string]string `json:"metadata"`
    Tags            []string          `json:"tags"`
}
```

#### 2. SemanticCacheConfig
```go
type SemanticCacheConfig struct {
    DefaultTTL          time.Duration `json:"default_ttl"`
    SimilarityThreshold float64       `json:"similarity_threshold"`
    MaxEntries         int           `json:"max_entries"`
    CleanupInterval     time.Duration `json:"cleanup_interval"`
    EnableVectorSearch  bool          `json:"enable_vector_search"`
    EnableFuzzyMatching bool          `json:"enable_fuzzy_matching"`
}
```

### Database Schema

#### semantic_cache_entries Table
```sql
CREATE TABLE semantic_cache_entries (
    id BIGINT GENERATED BY DEFAULT AS IDENTITY PRIMARY KEY,
    cache_key NVARCHAR(255) NOT NULL UNIQUE,
    prompt_hash NVARCHAR(64) NOT NULL,
    semantic_hash NVARCHAR(64) NOT NULL,
    model NVARCHAR(100) NOT NULL,
    domain NVARCHAR(100),
    prompt NCLOB,
    response NCLOB NOT NULL,
    tokens_used INTEGER,
    temperature DECIMAL(5,2),
    max_tokens INTEGER,
    similarity_score DECIMAL(3,2) DEFAULT 1.0,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    expires_at TIMESTAMP,
    access_count INTEGER DEFAULT 0,
    last_accessed TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    metadata NCLOB,
    tags NCLOB,
    INDEX idx_semantic_hash (semantic_hash),
    INDEX idx_similarity_score (similarity_score),
    INDEX idx_access_count (access_count)
)
```

## Integration

### Server Integration

The semantic cache is fully integrated into the VaultGemma server:

```go
type VaultGemmaServer struct {
    // ... other fields
    hanaCache     *storage.HANACache
    semanticCache *storage.SemanticCache
    // ... other fields
}
```

### Initialization

```go
// Create semantic cache with custom config
semanticConfig := &storage.SemanticCacheConfig{
    DefaultTTL:           24 * time.Hour,
    SimilarityThreshold: 0.8,
    MaxEntries:          10000,
    CleanupInterval:     1 * time.Hour,
    EnableVectorSearch:  false,
    EnableFuzzyMatching: true,
}
semanticCache := storage.NewSemanticCache(hanaPool, semanticConfig)

// Create tables
ctx := context.Background()
if err := semanticCache.CreateTables(ctx); err != nil {
    log.Printf("âš ï¸ Failed to create semantic cache tables: %v", err)
}
```

### Request Processing Flow

1. **Exact Cache Check**: First checks for exact cache hits
2. **Semantic Cache Check**: If no exact hit, searches for semantically similar responses
3. **Similarity Filtering**: Only returns responses above the similarity threshold
4. **Response Selection**: Uses the most similar cached response
5. **Cache Storage**: Stores new responses in both exact and semantic caches

## Usage Examples

### Basic Semantic Caching

```go
// Create semantic cache
cache := storage.NewSemanticCache(hanaPool, nil)

// Store a response
entry := &storage.SemanticCacheEntry{
    CacheKey:     "example-key",
    PromptHash:   "prompt-hash",
    SemanticHash: cache.GenerateSemanticHash("What is AI?"),
    Model:        "vaultgemma-1b",
    Domain:       "general",
    Prompt:       "What is AI?",
    Response:     "AI is artificial intelligence...",
    TokensUsed:   100,
    Temperature:  0.7,
    MaxTokens:    1000,
    SimilarityScore: 1.0,
    Tags: []string{"ai", "question"},
}

err := cache.Set(ctx, entry)
```

### Finding Similar Responses

```go
// Find semantically similar cached responses
similarEntries, err := cache.FindSemanticSimilar(ctx, 
    "What is artificial intelligence?", 
    "vaultgemma-1b", 
    "general", 
    0.8,  // 80% similarity threshold
    5,    // Limit to 5 results
)

if err == nil && len(similarEntries) > 0 {
    bestEntry := similarEntries[0]
    fmt.Printf("Found similar response: %s (similarity: %.2f)\n", 
        bestEntry.Response, 
        bestEntry.SimilarityScore,
    )
}
```

### Advanced Configuration

```go
// Custom semantic cache configuration
config := &storage.SemanticCacheConfig{
    DefaultTTL:           48 * time.Hour,        // 48 hour TTL
    SimilarityThreshold: 0.85,                   // 85% similarity threshold
    MaxEntries:          50000,                  // 50k max entries
    CleanupInterval:     2 * time.Hour,         // Cleanup every 2 hours
    EnableVectorSearch:  true,                   // Enable vector similarity
    EnableFuzzyMatching: true,                   // Enable fuzzy text matching
}

cache := storage.NewSemanticCache(hanaPool, config)
```

## Similarity Algorithms

### Semantic Hash Generation

The semantic cache uses advanced text processing to create semantic hashes:

1. **Text Normalization**: Converts to lowercase and trims whitespace
2. **Stop Word Removal**: Filters out common words (the, a, an, and, etc.)
3. **Word Filtering**: Removes words shorter than 3 characters
4. **Hash Generation**: Creates SHA256 hash of processed text

```go
func (sc *SemanticCache) GenerateSemanticHash(prompt string) string {
    // Normalize prompt
    normalized := strings.ToLower(strings.TrimSpace(prompt))
    
    // Remove stop words and filter
    words := strings.Fields(normalized)
    var filteredWords []string
    
    stopWords := map[string]bool{
        "the": true, "a": true, "an": true, "and": true, "or": true,
        // ... more stop words
    }
    
    for _, word := range words {
        if !stopWords[word] && len(word) > 2 {
            filteredWords = append(filteredWords, word)
        }
    }
    
    // Create semantic hash
    semanticText := strings.Join(filteredWords, " ")
    hash := sha256.Sum256([]byte(semanticText))
    return hex.EncodeToString(hash[:])
}
```

### Jaccard Similarity

For similarity scoring, the cache uses Jaccard similarity:

```go
func (sc *SemanticCache) calculateSimilarity(text1, text2 string) float64 {
    // Create word sets
    set1 := make(map[string]bool)
    set2 := make(map[string]bool)
    
    // Process texts and create sets
    // ... word processing logic
    
    // Calculate intersection and union
    intersection := 0
    for word := range set1 {
        if set2[word] {
            intersection++
        }
    }
    
    union := len(set1) + len(set2) - intersection
    
    if union == 0 {
        return 0.0
    }
    
    return float64(intersection) / float64(union)
}
```

## Performance Monitoring

### Cache Statistics

```go
// Get comprehensive cache statistics
stats, err := cache.GetStats(ctx)
if err != nil {
    log.Printf("Failed to get stats: %v", err)
    return
}

fmt.Printf("Total Entries: %d\n", stats.TotalEntries)
fmt.Printf("Hit Rate: %.2f%%\n", stats.HitRate*100)
fmt.Printf("Semantic Hit Rate: %.2f%%\n", stats.SemanticHitRate*100)
fmt.Printf("Average Similarity Score: %.2f\n", stats.AvgSimilarityScore)

// Model-specific statistics
for model, count := range stats.ByModel {
    fmt.Printf("Model %s: %d entries\n", model, count)
}

// Domain-specific statistics
for domain, count := range stats.ByDomain {
    fmt.Printf("Domain %s: %d entries\n", domain, count)
}
```

### Top Performing Entries

```go
// Get most accessed cache entries
topEntries, err := cache.GetTopEntries(ctx, 10)
if err != nil {
    log.Printf("Failed to get top entries: %v", err)
    return
}

for _, entry := range topEntries {
    fmt.Printf("Entry: %s (accesses: %d, similarity: %.2f)\n", 
        entry.Prompt, 
        entry.AccessCount, 
        entry.SimilarityScore,
    )
}
```

### Tag-based Analysis

```go
// Get entries by tags
entries, err := cache.GetByTags(ctx, []string{"ai", "question"}, 10)
if err != nil {
    log.Printf("Failed to get entries by tags: %v", err)
    return
}

for _, entry := range entries {
    fmt.Printf("Tagged entry: %s\n", entry.Prompt)
}
```

## Maintenance Operations

### Cleanup Operations

```go
// Clean up expired entries
err := cache.CleanupExpired(ctx)
if err != nil {
    log.Printf("Failed to cleanup expired entries: %v", err)
}

// Clean up old, rarely accessed entries
err = cache.CleanupOldEntries(ctx, 30, 5) // Remove entries older than 30 days with < 5 accesses
if err != nil {
    log.Printf("Failed to cleanup old entries: %v", err)
}
```

### Health Monitoring

```go
// Check cache health
stats, err := cache.GetStats(ctx)
if err != nil {
    log.Printf("Cache health check failed: %v", err)
    // Implement fallback or alerting
}

// Monitor hit rates
if stats.HitRate < 0.5 { // Less than 50% hit rate
    log.Printf("âš ï¸ Low cache hit rate: %.2f%%", stats.HitRate*100)
}

// Monitor semantic hit rates
if stats.SemanticHitRate < 0.1 { // Less than 10% semantic hit rate
    log.Printf("âš ï¸ Low semantic hit rate: %.2f%%", stats.SemanticHitRate*100)
}
```

## Best Practices

### Configuration Tuning

1. **Similarity Threshold**: Start with 0.8 and adjust based on performance
   - Higher threshold (0.9+): More precise matches, fewer false positives
   - Lower threshold (0.7-0.8): More matches, higher hit rate

2. **TTL Settings**: Balance between freshness and hit rate
   - Short TTL (1-6 hours): Fresh responses, lower hit rate
   - Long TTL (24-48 hours): Higher hit rate, potentially stale responses

3. **Max Entries**: Set based on available memory and storage
   - Monitor memory usage and adjust accordingly
   - Implement LRU eviction for memory-constrained environments

### Query Optimization

1. **Domain-specific Caching**: Use domain-specific similarity thresholds
2. **Model-specific Optimization**: Different models may benefit from different settings
3. **Tag-based Organization**: Use tags to improve cache organization and retrieval

### Monitoring and Alerting

1. **Hit Rate Monitoring**: Alert if hit rates drop below thresholds
2. **Performance Monitoring**: Track response times and similarity scores
3. **Storage Monitoring**: Monitor cache size and cleanup effectiveness
4. **Error Monitoring**: Track cache errors and failures

## Troubleshooting

### Common Issues

1. **Low Hit Rates**: 
   - Check similarity threshold settings
   - Verify prompt normalization
   - Review domain-specific configurations

2. **High Memory Usage**:
   - Reduce max entries
   - Implement more aggressive cleanup
   - Consider TTL adjustments

3. **Slow Similarity Searches**:
   - Check database indexes
   - Optimize similarity algorithms
   - Consider caching similarity calculations

### Debugging

```go
// Enable debug logging
log.SetLevel(log.DebugLevel)

// Check cache contents
topEntries, err := cache.GetTopEntries(ctx, 100)
if err != nil {
    log.Printf("Failed to get entries: %v", err)
}

// Analyze similarity patterns
for _, entry := range topEntries {
    log.Printf("Entry: %s, Similarity: %.2f, Accesses: %d", 
        entry.Prompt, 
        entry.SimilarityScore, 
        entry.AccessCount,
    )
}
```

## Future Enhancements

### Planned Features

- **Vector Similarity**: Integration with vector embeddings for more accurate similarity
- **Machine Learning**: ML-based similarity scoring and optimization
- **Real-time Analytics**: Live dashboards for cache performance
- **A/B Testing**: Framework for testing different similarity algorithms
- **Auto-tuning**: Automatic optimization of similarity thresholds

### Integration Opportunities

- **Vector Databases**: Integration with specialized vector databases
- **ML Pipelines**: Integration with machine learning pipelines for similarity
- **Analytics Platforms**: Export to analytics platforms for deeper insights
- **Monitoring Systems**: Integration with monitoring and alerting systems

## Conclusion

Semantic caching provides a powerful way to dramatically improve cache hit rates and reduce inference costs. By understanding the semantic meaning of queries rather than just exact matches, it can provide relevant responses even when the exact wording differs.

The system is designed to be highly configurable, performant, and maintainable, providing the foundation for production-ready semantic caching in AI inference systems.
