services:
  elasticsearch:
    image: docker.elastic.co/elasticsearch/elasticsearch:8.15.0
    container_name: elasticsearch
    restart: unless-stopped
    environment:
      - discovery.type=single-node
      - ES_JAVA_OPTS=-Xms512m -Xmx512m
      - xpack.security.enabled=false
      # Disable external inference services - use local models only
      # Note: External inference services (OpenAI, Anthropic, etc.) are disabled
      # All embeddings must use local inference endpoint (search-python:8091)
    ports:
      - "9200:9200"
      - "9300:9300"
    volumes:
      - esdata:/usr/share/elasticsearch/data
      # Mount elasticsearch config to disable external inference if needed
      # - ../../config/elasticsearch/elasticsearch.yml:/usr/share/elasticsearch/config/elasticsearch.yml:ro

  model-server:
    build:
      context: ../..
      dockerfile: services/localai/Dockerfile.model-server
    image: amodels/model-server:latest
    container_name: model-server
    restart: unless-stopped
    environment:
      - MODELS_BASE=/models
      - MODEL_SERVER_PORT=8088
      - PYTHONUNBUFFERED=1
    volumes:
      - models-data:/models:ro
    ports:
      - "8088:8088"
    networks:
      - models-network
    healthcheck:
      test: ["CMD", "python3", "-c", "import urllib.request; urllib.request.urlopen('http://localhost:8088/health')"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 10s
    labels:
      - "description=Model serving service - serves models via HTTP"

  models-storage:
    image: alpine:latest
    container_name: models-storage
    restart: unless-stopped
    volumes:
      - /home/aModels/models:/host-models:ro
      - models-data:/models:rw
    command: >
      sh -c "
      if [ ! -f /models/.initialized ]; then
        echo 'Initializing models-data volume from host...';
        if [ -d /host-models ] && [ \"\$(ls -A /host-models 2>/dev/null)\" ]; then
          echo 'Copying models from /host-models to /models...';
          cp -r /host-models/* /models/ 2>&1;
          echo 'Copy completed. Verifying...';
          MODEL_COUNT=\$(ls -1 /models | grep -v '^\.' | wc -l);
          echo \"Copied \${MODEL_COUNT} model directories\";
          touch /models/.initialized;
          echo 'Models initialized successfully';
        else
          echo 'WARNING: /host-models is empty or does not exist';
          touch /models/.initialized;
        fi;
      else
        echo 'Models already initialized';
      fi;
      tail -f /dev/null
      "
    networks:
      - models-network
    labels:
      - "description=Model storage service for LocalAI and transformers"

  transformers:
    build:
      context: ../..
      dockerfile: services/localai/Dockerfile.transformers
    image: amodels/transformers:latest
    container_name: transformers-service
    restart: unless-stopped
    environment:
      - TRANSFORMERS_CPU_PORT=9090
      - PYTHONUNBUFFERED=1
      - MODEL_SERVER_URL=http://model-server:8088
      - MODEL_CACHE_DIR=/tmp/models-cache
      - MODEL_CACHE_ENABLED=true
    volumes:
      - models-data:/models:ro
    ports:
      - "9090:9090"
    networks:
      - models-network
    depends_on:
      - model-server
      - models-storage
    healthcheck:
      test: ["CMD", "python3", "-c", "import urllib.request; urllib.request.urlopen('http://localhost:9090/health')"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s
    deploy:
      resources:
        reservations:
          devices:
            - capabilities: ["gpu"]

  localai:
    build:
      context: ../../..
      dockerfile: services/localai/Dockerfile
      args: []
    image: amodels/localai:vendored
    container_name: localai
    restart: unless-stopped
    environment:
      - MODELS_PATH=/models
      - EMBEDDINGS_MODEL=all-MiniLM-L6-v2
      - REDIS_URL=${REDIS_URL:-redis://redis:6379/0}
      - REDIS_PREFIX=localai
      - DEBUG=true
      - THREADS=4
      - CONTEXT_SIZE=512
      - PORT=8080
    volumes:
      - models-data:/models:ro
      - ../../services/localai/config/domains.json:/workspace/config/domains.json:ro
      - ../../services/localai/config/routing_rules.json:/workspace/config/routing_rules.json:ro
    ports:
      - "8081:8080"
    networks:
      - default
      - models-network
    depends_on:
      redis:
        condition: service_started
      postgres:
        condition: service_started
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080/healthz"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s
    deploy:
      resources:
        reservations:
          devices:
            - capabilities: ["gpu"]

  search-inference:
    build:
      context: ../..
      dockerfile: services/search/search-inference/Dockerfile
    image: amodels/search-inference:latest
    container_name: search-inference
    restart: unless-stopped
    environment:
      - SEARCH_CONFIG=/workspace/config/search-inference.yaml
      - ELASTICSEARCH_HOST=http://elasticsearch:9200
      # Model Configuration - LocalAI only (no external LLM dependencies)
      - LOCALAI_BASE_URL=http://localai:8080
      - LOCALAI_API_KEY=not-needed
      # Elasticsearch configuration - use local inference only
      - ELASTICSEARCH_ADDRESSES=http://elasticsearch:9200
    volumes:
      - ../../services/search/search-inference/config:/workspace/config:ro
      - ../../models:/workspace/models:ro
    ports:
      - "8090:8090"
    depends_on:
      - localai
      - elasticsearch
    deploy:
      resources:
        reservations:
          devices:
            - capabilities: ["gpu"]

  search-python:
    build:
      context: ../..
      dockerfile: legacy/stage3/search/python_service/Dockerfile
    image: amodels/search-python-service:latest
    container_name: search-python
    restart: unless-stopped
    environment:
      - PYTHONUNBUFFERED=1
    # Using baked image contents; mounts disabled to avoid host bind issues
    ports:
      - "8091:8091"
    depends_on:
      - search-inference

  trainer:
    build:
      context: ../..
      dockerfile: data/training/Dockerfile
    image: amodels/training-base:latest
    container_name: training-shell
    restart: unless-stopped
    command: 
      - /bin/bash
      - -c
      - |
        # Initialize testing directory
        # Note: Bind mount to /host-testing may not work due to overlay filesystem
        # Use './sync-testing.sh' or 'docker cp /home/aModels/testing/. training-shell:/workspace/testing/' to sync files
        if [ "$(ls -A /workspace/testing 2>/dev/null)" = "" ]; then
          echo "WARNING: /workspace/testing is empty"
          echo "To populate, run: ./infrastructure/docker/brev/sync-testing.sh"
        else
          echo "Testing directory has $(find /workspace/testing -type f | wc -l) files"
          # Bootstrap Python environment if needed
          if [ -f /workspace/testing/bootstrap_training_shell.sh ]; then
            bash /workspace/testing/bootstrap_training_shell.sh 2>/dev/null || true
          fi
        fi
        exec sleep infinity
    working_dir: /workspace
    volumes:
      - ../../tools/scripts:/workspace/scripts:rw
      - ../../config:/workspace/configs:rw
      - ../../models:/workspace/models:rw
      - ../../data:/workspace/data:rw
      - ../../tools:/workspace/tools:rw
      - /home/aModels/testing:/host-testing:ro
      - testing-files:/workspace/testing:rw
    deploy:
      resources:
        reservations:
          devices:
            - capabilities: ["gpu"]

  neo4j:
    image: neo4j:5.18-community
    container_name: neo4j
    restart: unless-stopped
    environment:
      - NEO4J_AUTH=neo4j/amodels123
      - NEO4J_PLUGINS=["apoc"]
      - NEO4J_server_default__listen__address=0.0.0.0
      - NEO4J_server_http_listen__address=0.0.0.0:7474
      - NEO4J_server_bolt_listen__address=0.0.0.0:7687
    ports:
      - "7474:7474"
      - "7687:7687"
    volumes:
      - neo4jdata:/data
      - neo4jlogs:/logs
    healthcheck:
      test: ["CMD", "cypher-shell", "-u", "neo4j", "-p", "amodels123", "RETURN 1"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 30s

  postgres:
    image: ankane/pgvector:latest
    container_name: postgres
    restart: unless-stopped
    environment:
      - POSTGRES_USER=postgres
      - POSTGRES_PASSWORD=postgres
      - POSTGRES_DB=amodels
    ports:
      - "5432:5432"
    volumes:
      - postgresdata:/var/lib/postgresql/data
      - ../../services/localai/migrations:/docker-entrypoint-initdb.d/localai:ro
      - ./init-scripts:/docker-entrypoint-initdb.d/init:ro
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U postgres"]
      interval: 10s
      timeout: 5s
      retries: 5

  redis:
    image: redis:7-alpine
    container_name: redis
    restart: unless-stopped
    command: redis-server --appendonly yes
    ports:
      - "6379:6379"
    volumes:
      - redisdata:/data

  config-sync:
    build:
      context: ../..
      dockerfile: services/localai/cmd/config-sync/Dockerfile
    image: amodels/config-sync:latest
    container_name: config-sync
    restart: unless-stopped
    environment:
      - POSTGRES_DSN=postgres://postgres:postgres@postgres:5432/amodels?sslmode=disable
      - REDIS_URL=redis://redis:6379/0
      - REDIS_DOMAIN_CONFIG_KEY=localai:domains:config
      - SYNC_INTERVAL=30s
    depends_on:
      postgres:
        condition: service_healthy
      redis:
        condition: service_started
    command: ["-postgres", "postgres://postgres:postgres@postgres:5432/amodels?sslmode=disable", "-redis", "redis://redis:6379/0", "-interval", "30s"]

  extract:
    build:
      context: ../../..
      dockerfile: ./services/extract/Dockerfile
    image: amodels/extract:latest
    container_name: extract-service
    restart: unless-stopped
    environment:
      - PORT=8082
      - GRPC_PORT=9090
      - TRAINING_OUTPUT_DIR=/workspace/data/training/extracts
      - NEO4J_URI=bolt://neo4j:7687
      - NEO4J_USERNAME=neo4j
      - NEO4J_PASSWORD=amodels123
      - POSTGRES_CATALOG_DSN=postgresql://postgres:postgres@postgres:5432/amodels?sslmode=disable
      - TELEMETRY_ENABLED=${TELEMETRY_ENABLED:-false}
      - LANGEXTRACT_API_KEY=${LANGEXTRACT_API_KEY:-}
      - SQLITE_PATH=/app/data/tables.db
      - DOCUMENT_STORE_PATH=/app/data/documents
      - POSTGRES_LANG_SERVICE_ADDR=${POSTGRES_LANG_SERVICE_ADDR:-}
      # OpenTelemetry Configuration
      - OTEL_TRACES_ENABLED=${OTEL_TRACES_ENABLED:-true}
      - OTEL_EXPORTER_TYPE=${OTEL_EXPORTER_TYPE:-otlp}
      - OTEL_EXPORTER_OTLP_ENDPOINT=${OTEL_EXPORTER_OTLP_ENDPOINT:-http://telemetry-exporter:4318}
      - OTEL_EXPORT_FILE_ENABLED=${OTEL_EXPORT_FILE_ENABLED:-true}
      - OTEL_EXPORT_FILE_PATH=/app/data/traces
      - OTEL_EXPORT_SIGNAVIO_ENABLED=${OTEL_EXPORT_SIGNAVIO_ENABLED:-false}
      - AGENT_FRAMEWORK_TYPE=extract
      # DeepAgents is enabled by default (10/10 integration)
      # Set DEEPAGENTS_ENABLED=false to disable
      - DEEPAGENTS_ENABLED=${DEEPAGENTS_ENABLED:-true}
      - DEEPAGENTS_URL=${DEEPAGENTS_URL:-http://deepagents-service:9004}
      # Domain Detection - LocalAI integration for domain association during extraction
      - LOCALAI_URL=http://localai-compat:8080
      # Model Configuration - LocalAI only (no external LLM dependencies)
      # langextract-api: Disabled by default (empty URL = disabled)
      # To enable with LocalAI: set LANGEXTRACT_API_URL=http://localai:8081/v1
      # See config/langextract-config.md for details
      - LANGEXTRACT_API_URL=${LANGEXTRACT_API_URL:-}
      - LANGEXTRACT_API_KEY=${LANGEXTRACT_API_KEY:-}
      # Multimodal Extraction and DeepSeek OCR (Phase 6)
      - USE_MULTIMODAL_EXTRACTION=${USE_MULTIMODAL_EXTRACTION:-true}
      - USE_DEEPSEEK_OCR=${USE_DEEPSEEK_OCR:-true}
      # DMS Integration removed - now handled by Extract service
    volumes:
      - ../../data:/workspace/data:rw
      - ../../logs:/workspace/logs:rw
    ports:
      - "8083:8082"
    depends_on:
      elasticsearch:
        condition: service_started
      neo4j:
        condition: service_healthy
      postgres:
        condition: service_healthy

  graph:
    build:
      context: ../../..
      dockerfile: services/graph/Dockerfile
    image: amodels/graph-server:latest
    container_name: graph-server
    restart: unless-stopped
    environment:
      - GRAPH_CONFIG=/workspace/config/graph.yaml
      - NEO4J_URI=bolt://neo4j:7687
      - NEO4J_USERNAME=neo4j
      - NEO4J_PASSWORD=amodels123
      # Model Configuration - LocalAI only (no external LLM dependencies)
      - LOCALAI_URL=http://localai:8081
      # OpenTelemetry Configuration
      - OTEL_TRACES_ENABLED=${OTEL_TRACES_ENABLED:-true}
      - OTEL_EXPORTER_TYPE=${OTEL_EXPORTER_TYPE:-otlp}
      - OTEL_EXPORTER_OTLP_ENDPOINT=${OTEL_EXPORTER_OTLP_ENDPOINT:-http://telemetry-exporter:4318}
      - OTEL_EXPORT_FILE_ENABLED=${OTEL_EXPORT_FILE_ENABLED:-true}
      - OTEL_EXPORT_FILE_PATH=/app/data/traces
      - OTEL_EXPORT_SIGNAVIO_ENABLED=${OTEL_EXPORT_SIGNAVIO_ENABLED:-false}
      - AGENT_FRAMEWORK_TYPE=langgraph
    volumes:
      - ../../config:/workspace/config:ro
      - ../../artifacts/model-release:/workspace/assets:ro
    ports:
      - "8080:8080"
      - "19080:19080"
    depends_on:
      extract:
        condition: service_started
      neo4j:
        condition: service_healthy
    deploy:
      resources:
        reservations:
          devices:
            - capabilities: ["gpu"]

  deepagents:
    build:
      context: ../../..
      dockerfile: services/deepagents/Dockerfile
    image: amodels/deepagents:latest
    container_name: deepagents-service
    restart: unless-stopped
    environment:
      - DEEPAGENTS_PORT=9004
      # Service URLs for DeepAgents tools
      - EXTRACT_SERVICE_URL=http://extract-service:8082
      # OpenTelemetry Configuration
      - OTEL_TRACES_ENABLED=${OTEL_TRACES_ENABLED:-true}
      - OTEL_EXPORTER_OTLP_ENDPOINT=${OTEL_EXPORTER_OTLP_ENDPOINT:-http://telemetry-exporter:4318}
      - AGENT_FRAMEWORK_TYPE=deepagents
      - AGENTFLOW_SERVICE_URL=http://agentflow-service:8001
      - GRAPH_SERVICE_URL=http://graph-server:8080
      # Model Configuration - LocalAI only (no external LLM dependencies)
      - LOCALAI_URL=http://localai:8081
      # Domain name (defaults to "general" which uses phi-3.5-mini via transformers backend)
      # Available domains can be queried at http://localai:8081/v1/domains
      - LOCALAI_MODEL=${LOCALAI_MODEL:-general}
      # Disable external LLM providers
      - ANTHROPIC_API_KEY=
      - OPENAI_API_KEY=
    ports:
      - "9004:9004"
    depends_on:
      - postgres
      - redis
      - localai
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:9004/healthz"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s

  localai-compat:
    build:
      context: ../../..
      dockerfile: services/localai/shim/Dockerfile
    image: amodels/localai-compat:latest
    container_name: localai-compat
    environment:
      - LOCALAI_CORE_URL=http://localai:8080
      - REDIS_URL=redis://redis:6379/0
      - REDIS_DOMAIN_CONFIG_KEY=localai:domains:config
    depends_on:
      localai:
        condition: service_started
    ports:
      - "8082:8080"

  catalog:
    build:
      context: ../../..
      dockerfile: services/catalog/Dockerfile
    image: amodels/catalog:latest
    container_name: catalog
    restart: unless-stopped
    environment:
      - NEO4J_URI=bolt://neo4j:7687
      - NEO4J_USERNAME=neo4j
      - NEO4J_PASSWORD=amodels123
      - REDIS_URL=redis://redis:6379/0
      - POSTGRES_DSN=postgresql://postgres:postgres@postgres:5432/amodels?sslmode=disable
    ports:
      - "8084:8084"
    depends_on:
      neo4j:
        condition: service_healthy
      redis:
        condition: service_started
      postgres:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "wget", "--no-verbose", "--tries=1", "--spider", "http://localhost:8084/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s

  training-service:
    build:
      context: ../../..
      dockerfile: services/training/Dockerfile
    image: amodels/training-service:latest
    container_name: training-service
    restart: unless-stopped
    environment:
      - PORT=8080
      - HOST=0.0.0.0
      - EXTRACT_SERVICE_URL=http://extract-service:8082
      - LOCALAI_URL=http://localai-compat:8080
      - POSTGRES_DSN=postgresql://postgres:postgres@postgres:5432/amodels?sslmode=disable
      - REDIS_URL=redis://redis:6379/0
      - TRAINING_OUTPUT_DIR=/workspace/data/training
      # Optional: Enable deep learning pattern learning
      - USE_GNN_PATTERNS=${USE_GNN_PATTERNS:-false}
      - USE_TRANSFORMER_SEQUENCES=${USE_TRANSFORMER_SEQUENCES:-false}
      - USE_META_PATTERNS=${USE_META_PATTERNS:-false}
      - USE_ACTIVE_LEARNING=${USE_ACTIVE_LEARNING:-false}
    volumes:
      - ../../data:/workspace/data:rw
      - ../../logs:/workspace/logs:rw
    ports:
      - "8085:8080"
    depends_on:
      - postgres
      - redis
      - localai-compat
      - extract
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s

  # DMS service removed - functionality migrated to Extract service with Gitea storage
  # Document management is now handled by Extract service endpoints:
  # - POST /documents/upload
  # - GET /documents
  # - GET /documents/{id}

  telemetry-exporter:
    build:
      context: ../../..
      dockerfile: ./services/telemetry-exporter/Dockerfile
    image: amodels/telemetry-exporter:latest
    container_name: telemetry-exporter
    restart: unless-stopped
    environment:
      - OTEL_EXPORT_MODE=${OTEL_EXPORT_MODE:-both}
      - OTEL_EXPORT_FLUSH_INTERVAL=${OTEL_EXPORT_FLUSH_INTERVAL:-30s}
      - OTEL_EXPORT_FILE_ENABLED=${OTEL_EXPORT_FILE_ENABLED:-true}
      - OTEL_EXPORT_FILE_PATH=/app/data/traces
      - OTEL_EXPORT_FILE_MAX_SIZE=${OTEL_EXPORT_FILE_MAX_SIZE:-104857600}
      - OTEL_EXPORT_FILE_MAX_FILES=${OTEL_EXPORT_FILE_MAX_FILES:-10}
      - OTEL_EXPORT_SIGNAVIO_ENABLED=${OTEL_EXPORT_SIGNAVIO_ENABLED:-false}
      - SIGNAVIO_API_URL=${SIGNAVIO_API_URL:-}
      - SIGNAVIO_API_KEY=${SIGNAVIO_API_KEY:-}
      - SIGNAVIO_TENANT_ID=${SIGNAVIO_TENANT_ID:-}
      - SIGNAVIO_DATASET=${SIGNAVIO_DATASET:-}
      - SIGNAVIO_BATCH_SIZE=${SIGNAVIO_BATCH_SIZE:-100}
      - SIGNAVIO_TIMEOUT=${SIGNAVIO_TIMEOUT:-30s}
      - SIGNAVIO_MAX_RETRIES=${SIGNAVIO_MAX_RETRIES:-3}
    volumes:
      - ../../data/traces:/app/data/traces
    ports:
      - "8087:8080"
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 10s

  gitea:
    image: gitea/gitea:1.21
    container_name: gitea
    restart: unless-stopped
    environment:
      - USER_UID=1000
      - USER_GID=1000
      - GITEA__database__DB_TYPE=postgres
      - GITEA__database__HOST=postgres:5432
      - GITEA__database__NAME=gitea
      - GITEA__database__USER=gitea
      - GITEA__database__PASSWD=gitea_password
      - GITEA__server__DOMAIN=localhost
      - GITEA__server__SSH_DOMAIN=localhost
      - GITEA__server__ROOT_URL=http://localhost:3000/
      - GITEA__server__HTTP_PORT=3000
      - GITEA__server__SSH_PORT=2223
      - GITEA__server__DISABLE_SSH=false
      - GITEA__security__INSTALL_LOCK=true
      - GITEA__service__DISABLE_REGISTRATION=false
      - GITEA__repository__DEFAULT_BRANCH=main
    ports:
      - "3000:3000"
      - "2223:22"
    volumes:
      - gitea-data:/data
      - /etc/timezone:/etc/timezone:ro
      - /etc/localtime:/etc/localtime:ro
    depends_on:
      - postgres
    networks:
      - default
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:3000/api/healthz"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 60s
    labels:
      - "description=Gitea Git service for code storage and version control"

networks:
  default:
    driver: bridge
  models-network:
    name: models-network
    driver: bridge

volumes:
  esdata:
  neo4jdata:
  neo4jlogs:
  postgresdata:
  redisdata:
  localai-config:
  testing-files:
  gitea-data:
  models-data:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: /home/aModels/models
