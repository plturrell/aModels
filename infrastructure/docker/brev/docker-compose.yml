services:
  elasticsearch:
    image: docker.elastic.co/elasticsearch/elasticsearch:8.15.0
    container_name: elasticsearch
    restart: unless-stopped
    environment:
      - discovery.type=single-node
      - ES_JAVA_OPTS=-Xms512m -Xmx512m
      - xpack.security.enabled=false
      # Disable external inference services - use local models only
      # Note: External inference services (OpenAI, Anthropic, etc.) are disabled
      # All embeddings must use local inference endpoint (search-python:8091)
    ports:
      - "9200:9200"
      - "9300:9300"
    volumes:
      - esdata:/usr/share/elasticsearch/data
      # Mount elasticsearch config to disable external inference if needed
      # - ../../configs/elasticsearch/elasticsearch.yml:/usr/share/elasticsearch/config/elasticsearch.yml:ro

  transformers:
    build:
      context: ../..
      dockerfile: services/localai/Dockerfile.transformers
    image: amodels/transformers:latest
    container_name: transformers-service
    restart: unless-stopped
    environment:
      - TRANSFORMERS_CPU_PORT=9090
      - PYTHONUNBUFFERED=1
    volumes:
      - ../../models:/models:ro
    ports:
      - "9090:9090"
    healthcheck:
      test: ["CMD", "python3", "-c", "import urllib.request; urllib.request.urlopen('http://localhost:9090/health')"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s
    deploy:
      resources:
        reservations:
          devices:
            - capabilities: ["gpu"]

  localai:
    build:
      context: ../..
      dockerfile: services/localai/Dockerfile
    image: amodels/localai:latest
    container_name: localai
    restart: unless-stopped
    environment:
      - MODELS_DIR=/models
      - TRANSFORMERS_BASE_URL=http://search-python:8091
      - TRANSFORMERS_MODEL=all-MiniLM-L6-v2
      - DISABLE_VAULTGEMMA_FALLBACK=1
      # Redis config (preferred over file-based)
      - REDIS_URL=${REDIS_URL:-redis://redis:6379/0}
      - REDIS_DOMAIN_CONFIG_KEY=${REDIS_DOMAIN_CONFIG_KEY:-localai:domains:config}
      # Enable domains that require env vars
      - ENABLE_GEMMA7B=1
      # Fallback to file if Redis not available
      - DOMAIN_CONFIG_PATH=/workspace/config/domains.json
    volumes:
      - ../../models:/models:ro
      - ../../services/localai/config:/workspace/config:ro
    ports:
      - "8081:8080"
    depends_on:
      transformers:
        condition: service_healthy
      redis:
        condition: service_started
      postgres:
        condition: service_started
    deploy:
      resources:
        reservations:
          devices:
            - capabilities: ["gpu"]

  search-inference:
    build:
      context: ../..
      dockerfile: services/search/search-inference/Dockerfile
    image: amodels/search-inference:latest
    container_name: search-inference
    restart: unless-stopped
    environment:
      - SEARCH_CONFIG=/workspace/config/search-inference.yaml
      - ELASTICSEARCH_HOST=http://elasticsearch:9200
      # Model Configuration - LocalAI only (no external LLM dependencies)
      - LOCALAI_BASE_URL=http://localai:8080
      - LOCALAI_API_KEY=not-needed
      # Elasticsearch configuration - use local inference only
      - ELASTICSEARCH_ADDRESSES=http://elasticsearch:9200
    volumes:
      - ../../services/search/search-inference/config:/workspace/config:ro
      - ../../models:/workspace/models:ro
    ports:
      - "8090:8090"
    depends_on:
      - localai
      - elasticsearch
    deploy:
      resources:
        reservations:
          devices:
            - capabilities: ["gpu"]

  search-python:
    build:
      context: ../..
      dockerfile: legacy/stage3/search/python_service/Dockerfile
    image: amodels/search-python-service:latest
    container_name: search-python
    restart: unless-stopped
    environment:
      - PYTHONUNBUFFERED=1
    # Using baked image contents; mounts disabled to avoid host bind issues
    ports:
      - "8091:8091"
    depends_on:
      - search-inference

  trainer:
    build:
      context: ../..
      dockerfile: data/training/Dockerfile
    image: amodels/training-base:latest
    container_name: training-shell
    restart: unless-stopped
    command: ["sleep", "infinity"]
    working_dir: /workspace
    volumes:
      - ../../tools/scripts:/workspace/scripts:rw
      - ../../configs:/workspace/configs:rw
      - ../../models:/workspace/models:rw
      - ../../data:/workspace/data:rw
      - ../../tools:/workspace/tools:rw
    deploy:
      resources:
        reservations:
          devices:
            - capabilities: ["gpu"]

  neo4j:
    image: neo4j:5.18-community
    container_name: neo4j
    restart: unless-stopped
    environment:
      - NEO4J_AUTH=neo4j/amodels123
      - NEO4J_PLUGINS=["apoc"]
      - NEO4J_server_default__listen__address=0.0.0.0
      - NEO4J_server_http_listen__address=0.0.0.0:7474
      - NEO4J_server_bolt_listen__address=0.0.0.0:7687
    ports:
      - "7474:7474"
      - "7687:7687"
    volumes:
      - neo4jdata:/data
      - neo4jlogs:/logs

  postgres:
    image: ankane/pgvector:latest
    container_name: postgres
    restart: unless-stopped
    environment:
      - POSTGRES_USER=postgres
      - POSTGRES_PASSWORD=postgres
      - POSTGRES_DB=amodels
    ports:
      - "5432:5432"
    volumes:
      - postgresdata:/var/lib/postgresql/data
      - ../../services/localai/migrations:/docker-entrypoint-initdb.d:ro
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U postgres"]
      interval: 10s
      timeout: 5s
      retries: 5

  redis:
    image: redis:7-alpine
    container_name: redis
    restart: unless-stopped
    command: redis-server --appendonly yes
    ports:
      - "6379:6379"
    volumes:
      - redisdata:/data

  config-sync:
    build:
      context: ../..
      dockerfile: services/localai/cmd/config-sync/Dockerfile
    image: amodels/config-sync:latest
    container_name: config-sync
    restart: unless-stopped
    environment:
      - POSTGRES_DSN=postgres://postgres:postgres@postgres:5432/amodels?sslmode=disable
      - REDIS_URL=redis://redis:6379/0
      - REDIS_DOMAIN_CONFIG_KEY=localai:domains:config
      - SYNC_INTERVAL=30s
    depends_on:
      postgres:
        condition: service_healthy
      redis:
        condition: service_started
    command: ["-postgres", "postgres://postgres:postgres@postgres:5432/amodels?sslmode=disable", "-redis", "redis://redis:6379/0", "-interval", "30s"]

  extract:
    build:
      context: ../..
      dockerfile: services/extract/Dockerfile
    image: amodels/extract:latest
    container_name: extract-service
    restart: unless-stopped
    environment:
      - PORT=8082
      - GRPC_PORT=9090
      - TRAINING_OUTPUT_DIR=/workspace/data/training/extracts
      - NEO4J_URI=bolt://neo4j:7687
      - NEO4J_USERNAME=neo4j
      - NEO4J_PASSWORD=amodels123
      - POSTGRES_CATALOG_DSN=postgresql://postgres:postgres@postgres:5432/amodels?sslmode=disable
      # DeepAgents is enabled by default (10/10 integration)
      # Set DEEPAGENTS_ENABLED=false to disable
      - DEEPAGENTS_ENABLED=${DEEPAGENTS_ENABLED:-true}
      - DEEPAGENTS_URL=${DEEPAGENTS_URL:-http://deepagents-service:9004}
      # Domain Detection - LocalAI integration for domain association during extraction
      - LOCALAI_URL=${LOCALAI_URL:-http://localai:8080}
      # Model Configuration - LocalAI only (no external LLM dependencies)
      # langextract-api: Disabled by default (empty URL = disabled)
      # To enable with LocalAI: set LANGEXTRACT_API_URL=http://localai:8081/v1
      # See configs/langextract-config.md for details
      - LANGEXTRACT_API_URL=${LANGEXTRACT_API_URL:-}
      - LANGEXTRACT_API_KEY=${LANGEXTRACT_API_KEY:-}
    volumes:
      - ../../data:/workspace/data:rw
      - ../../logs:/workspace/logs:rw
    ports:
      - "8082:8082"
      - "9090:9090"
      - "8815:8815"
    depends_on:
      - elasticsearch
      - neo4j
      - postgres

  graph:
    build:
      context: ../..
      dockerfile: services/graph/Dockerfile
    image: amodels/graph-server:latest
    container_name: graph-server
    restart: unless-stopped
    environment:
      - GRAPH_CONFIG=/workspace/config/graph.yaml
      - NEO4J_URI=bolt://neo4j:7687
      - NEO4J_USERNAME=neo4j
      - NEO4J_PASSWORD=amodels123
      # Model Configuration - LocalAI only (no external LLM dependencies)
      - LOCALAI_URL=http://localai:8081
    volumes:
      - ../../configs:/workspace/config:ro
      - ../../artifacts/model-release:/workspace/assets:ro
    ports:
      - "8080:8080"
      - "19080:19080"
    depends_on:
      - extract
      - neo4j
    deploy:
      resources:
        reservations:
          devices:
            - capabilities: ["gpu"]

  deepagents:
    build:
      context: ../../..
      dockerfile: services/deepagents/Dockerfile
    image: amodels/deepagents:latest
    container_name: deepagents-service
    restart: unless-stopped
    environment:
      - DEEPAGENTS_PORT=9004
      # Service URLs for DeepAgents tools
      - EXTRACT_SERVICE_URL=http://extract-service:8082
      - AGENTFLOW_SERVICE_URL=http://agentflow-service:8001
      - GRAPH_SERVICE_URL=http://graph-server:8080
      # Model Configuration - LocalAI only (no external LLM dependencies)
      - LOCALAI_URL=http://localai:8081
      # Domain name (defaults to "general" which uses phi-3.5-mini via transformers backend)
      # Available domains can be queried at http://localai:8081/v1/domains
      - LOCALAI_MODEL=${LOCALAI_MODEL:-general}
      # Disable external LLM providers
      - ANTHROPIC_API_KEY=
      - OPENAI_API_KEY=
    ports:
      - "9004:9004"
    depends_on:
      - postgres
      - redis
      - localai
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:9004/healthz"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s

networks:
  default:
    driver: bridge

volumes:
  esdata:
  neo4jdata:
  neo4jlogs:
  postgresdata:
  redisdata:
