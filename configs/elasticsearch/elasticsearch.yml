# Elasticsearch Configuration for Local Models Only
# This configuration ensures Elasticsearch uses only local inference services

# Disable external inference service providers
# Note: This prevents Elasticsearch from using OpenAI, Anthropic, or other external APIs
# All inference must go through local inference endpoint (search-python:8091)

# Inference service configuration
# If Elasticsearch inference plugin is enabled, configure it to use local endpoint
# Uncomment and configure if you need Elasticsearch native inference:
# xpack.ml.inference:
#   services:
#     local_embeddings:
#       service: http://search-python:8091
#       model: all-MiniLM-L6-v2

# Alternative: Disable inference plugin entirely if not needed
# xpack.ml.enabled: false

# Note: The search-inference service already handles embeddings via LocalAI
# This Elasticsearch instance is primarily for document indexing and retrieval
# Vector embeddings are generated by search-inference service and stored in Elasticsearch

